{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02 - Hello Spark.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPgQ99d4SAeVmhRZj5Tawf0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QAM008wB-Ow5"},"source":["# History of Spark"]},{"cell_type":"markdown","metadata":{"id":"ETYWL10J-QQA"},"source":["## grep\n","- Tutorial: https://www.cyberciti.biz/faq/howto-use-grep-command-in-linux-unix/"]},{"cell_type":"markdown","metadata":{"id":"wkYQ2RI3-R4B"},"source":["## Hadoop\n","- MapReduce (2004)\n","- Hadoop @Yahoo (2006)\n","- \"Take the processing to the data\""]},{"cell_type":"markdown","metadata":{"id":"8WX1fjdc9Ust"},"source":["<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0e/Hadoop_logo.svg\" width=\"60%\"/>"]},{"cell_type":"markdown","metadata":{"id":"nb1YYzFOzqMO"},"source":["https://phoenixnap.com/kb/hadoop-vs-spark"]},{"cell_type":"markdown","metadata":{"id":"gwqCvH8t-T-g"},"source":["## [Spark](http://spark.apache.org/)\n","- Spark @UC Berkley (2009)\n","- Open Source Spark, BSD License (2010)\n","- Developed by AmpLab (2011)\n","- Databricks maintains open source Spark (2013) => Stability\n","- License change to Apache Commons 2.0 (2013)\n","- [Large Scale Sorting record](https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html) (2014)\n","- Spark 2.x (2016)\n","- Spark 3.x (2020)"]},{"cell_type":"markdown","metadata":{"id":"U_BdYQnyIfBj"},"source":["## Languages"]},{"cell_type":"markdown","metadata":{"id":"DkG0YCMoIgaZ"},"source":["### [Scala](https://www.scala-lang.org/)\n","- Designed by Martin Odersky (2004)\n","- Developed by EPFL (Switzerland)"]},{"cell_type":"markdown","metadata":{"id":"LsRq0bPOI-j_"},"source":["### [Java](https://docs.oracle.com/javase/8/docs/technotes/guides/language/index.html)\n","  - Designed by James Gosling (1995)\n","  - Developed by Oracle"]},{"cell_type":"markdown","metadata":{"id":"RH4yO0yYJr_3"},"source":["### [Python](https://www.python.org/)\n","- Designed by Guido van Rossum (1991)\n","- Developed by Python Software Foundation"]},{"cell_type":"markdown","metadata":{"id":"P7ZCagFPWGU9"},"source":["### [R](https://www.r-project.org/)\n","  - Designed by Ross Ihaka and Robert Gentleman (1993)\n","  - Developed by R Core Team"]},{"cell_type":"markdown","metadata":{"id":"JW84Gl_BX9rA"},"source":["### [C#](https://docs.microsoft.com/en-us/dotnet/csharp/)\n","- Designed by Anders Hejlsberg (2000)\n","- Developed by Microsoft"]},{"cell_type":"markdown","metadata":{"id":"M_HrW8VEYZ5W"},"source":["### [F#](https://docs.microsoft.com/en-us/dotnet/fsharp/)\n","- Designed by Don Syme (2005)\n","- Developed by Microsoft and The F# Software Foundation"]},{"cell_type":"markdown","metadata":{"id":"mXz9nSUXY3pe"},"source":["### SQL\n","- Designed by Donald D. Chamberlin and Raymond F. Boyce (1974)\n","- Developed by \tISO/IEC"]},{"cell_type":"markdown","metadata":{"id":"7nqSBGLm7wbx"},"source":["## Libraries"]},{"cell_type":"markdown","metadata":{"id":"Fvl3Ne9f7yDZ"},"source":["### SQL"]},{"cell_type":"markdown","metadata":{"id":"v_mwjm1J7zxi"},"source":["### Streaming"]},{"cell_type":"markdown","metadata":{"id":"dBeGpwrk705R"},"source":["### MLlib"]},{"cell_type":"markdown","metadata":{"id":"GvNGU_8y-q-a"},"source":["### GraphX"]},{"cell_type":"markdown","metadata":{"id":"RBFqiNp0FWvy"},"source":["# Data"]},{"cell_type":"code","metadata":{"id":"CqPPuhPMFe3I","executionInfo":{"status":"ok","timestamp":1636389943819,"user_tz":-60,"elapsed":14,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["from google.colab import drive\n","import os\n","from requests import get"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AAyTUG0S86e","executionInfo":{"status":"ok","timestamp":1636389943820,"user_tz":-60,"elapsed":13,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"1218e14a-7f1a-4090-8e7b-9d63e452f54a"},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"id":"p-wwK3yHI8RO","executionInfo":{"status":"ok","timestamp":1636389943821,"user_tz":-60,"elapsed":8,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"ff295b6c-3fdb-493f-a334-219614586b36"},"source":["%%html\n","<iframe src=\"https://corpus.canterbury.ac.nz/descriptions/\" width=\"800\" height=\"600\"></iframe>"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<iframe src=\"https://corpus.canterbury.ac.nz/descriptions/\" width=\"800\" height=\"600\"></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iTvSte7UFYa6","executionInfo":{"status":"ok","timestamp":1636390079291,"user_tz":-60,"elapsed":135476,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"a2850945-15bc-403e-e749-b5517064fe6e"},"source":["drive.mount('/content/gdrive', force_remount=True)\n","dir = os.path.join('gdrive', 'My Drive', 'Eurostat', '05 - Data Science for Big Data')\n","data_dir = os.path.join(dir, 'data')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"ebo3bRysJEtN","executionInfo":{"status":"ok","timestamp":1636390079291,"user_tz":-60,"elapsed":7,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["def download_save(url, filename):\n","  res = get(url)\n","  if res.status_code != 200:\n","    print(f\"Couldn't fetch data from {url}\")\n","  else:\n","    csv_file = open(os.path.join(data_dir, filename), 'wb')\n","    csv_file.write(res.content)\n","    csv_file.close()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNdnrf77I_ek","executionInfo":{"status":"ok","timestamp":1636390095726,"user_tz":-60,"elapsed":16439,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["download_save('http://corpus.canterbury.ac.nz/resources/large.zip',\n","              'large.zip')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qypEMQUjbum","executionInfo":{"status":"ok","timestamp":1636390095727,"user_tz":-60,"elapsed":28,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"94853aeb-d6a4-49fb-dbd1-0059c28a5896"},"source":["!cd \"gdrive/MyDrive/Eurostat/05 - Data Science for Big Data/data\" && unzip large.zip"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  large.zip\n","  inflating: bible.txt               \n","  inflating: E.coli                  \n","  inflating: world192.txt            \n"]}]},{"cell_type":"markdown","metadata":{"id":"KkgipyhlYZ_F"},"source":["# Hello Spark"]},{"cell_type":"markdown","metadata":{"id":"17iurFg18Ntr"},"source":["## Pyspark Installation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4opDI-aHMWxg","executionInfo":{"status":"ok","timestamp":1636390146314,"user_tz":-60,"elapsed":50236,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"11bf8bca-618f-493d-92d1-bb91e056b42c"},"source":["!pip install pyspark"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n","\u001b[K     |████████████████████████████████| 281.3 MB 36 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.2\n","  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 15.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=0c1a52dbe7530ea5c8552e185fbb5a64a26be9fb21158ba46a9c4de66134595a\n","  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"8ZnteAiYcLrt"},"source":["## Spark Context"]},{"cell_type":"code","metadata":{"id":"CfqSxmLaOKMt","executionInfo":{"status":"ok","timestamp":1636390152107,"user_tz":-60,"elapsed":5796,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["from pyspark import SparkConf, SparkContext\n","\n","conf = SparkConf().setAppName(\"Hello-Spark\").setMaster(\"local[*]\")\n","sc = SparkContext(conf=conf)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"7hT-hdwYPomr","executionInfo":{"status":"ok","timestamp":1636390152107,"user_tz":-60,"elapsed":22,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"2d27a81d-5560-4c10-8ecc-b5e28e0470bf"},"source":["sc"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://003ab74d17f3:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.2.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Hello-Spark</code></dd>\n","            </dl>\n","        </div>\n","        "],"text/plain":["<SparkContext master=local[*] appName=Hello-Spark>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"IXgIh6yVkSwe","executionInfo":{"status":"ok","timestamp":1636390152954,"user_tz":-60,"elapsed":851,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["file_name = os.path.join(data_dir, 'bible.txt')\n","bible_file = sc.textFile(file_name)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhZe1F1BXhPj","executionInfo":{"status":"ok","timestamp":1636390153466,"user_tz":-60,"elapsed":516,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"1110d96e-0da6-42c6-d25c-3694ae6ae89e"},"source":["help(bible_file)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on RDD in module pyspark.rdd object:\n","\n","class RDD(builtins.object)\n"," |  RDD(jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n"," |  \n"," |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n"," |  Represents an immutable, partitioned collection of elements that can be\n"," |  operated on in parallel.\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __add__(self, other)\n"," |      Return the union of this RDD and another one.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n"," |      >>> (rdd + rdd).collect()\n"," |      [1, 1, 2, 3, 1, 1, 2, 3]\n"," |  \n"," |  __getnewargs__(self)\n"," |  \n"," |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  aggregate(self, zeroValue, seqOp, combOp)\n"," |      Aggregate the elements of each partition, and then the results for all\n"," |      the partitions, using a given combine functions and a neutral \"zero\n"," |      value.\"\n"," |      \n"," |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n"," |      as its result value to avoid object allocation; however, it should not\n"," |      modify ``t2``.\n"," |      \n"," |      The first function (seqOp) can return a different result type, U, than\n"," |      the type of this RDD. Thus, we need one operation for merging a T into\n"," |      an U and one operation for merging two U\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n"," |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n"," |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n"," |      (10, 4)\n"," |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n"," |      (0, 0)\n"," |  \n"," |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash at 0x7f09c77efcb0>)\n"," |      Aggregate the values of each key, using given combine functions and a neutral\n"," |      \"zero value\". This function can return a different result type, U, than the type\n"," |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n"," |      a U and one operation for merging two U's, The former operation is used for merging\n"," |      values within a partition, and the latter is used for merging values between\n"," |      partitions. To avoid memory allocation, both of these functions are\n"," |      allowed to modify and return their first argument instead of creating a new U.\n"," |  \n"," |  barrier(self)\n"," |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n"," |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n"," |      entire stage and relaunch all tasks for this stage.\n"," |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n"," |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n"," |      \n"," |      .. versionadded:: 2.4.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`RDDBarrier`\n"," |          instance that provides actions within a barrier stage.\n"," |      \n"," |      See Also\n"," |      --------\n"," |      pyspark.BarrierTaskContext\n"," |      \n"," |      Notes\n"," |      -----\n"," |      For additional information see\n"," |      \n"," |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n"," |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n"," |      \n"," |      This API is experimental\n"," |  \n"," |  cache(self)\n"," |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n"," |  \n"," |  cartesian(self, other)\n"," |      Return the Cartesian product of this RDD and another one, that is, the\n"," |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n"," |      ``b`` is in `other`.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 2])\n"," |      >>> sorted(rdd.cartesian(rdd).collect())\n"," |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n"," |  \n"," |  checkpoint(self)\n"," |      Mark this RDD for checkpointing. It will be saved to a file inside the\n"," |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n"," |      all references to its parent RDDs will be removed. This function must\n"," |      be called before any job has been executed on this RDD. It is strongly\n"," |      recommended that this RDD is persisted in memory, otherwise saving it\n"," |      on a file will require recomputation.\n"," |  \n"," |  coalesce(self, numPartitions, shuffle=False)\n"," |      Return a new RDD that is reduced into `numPartitions` partitions.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n"," |      [[1], [2, 3], [4, 5]]\n"," |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n"," |      [[1, 2, 3, 4, 5]]\n"," |  \n"," |  cogroup(self, other, numPartitions=None)\n"," |      For each key k in `self` or `other`, return a resulting RDD that\n"," |      contains a tuple with the list of values for that key in `self` as\n"," |      well as `other`.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n"," |      >>> y = sc.parallelize([(\"a\", 2)])\n"," |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n"," |      [('a', ([1], [2])), ('b', ([4], []))]\n"," |  \n"," |  collect(self)\n"," |      Return a list that contains all of the elements in this RDD.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This method should only be used if the resulting array is expected\n"," |      to be small, as all the data is loaded into the driver's memory.\n"," |  \n"," |  collectAsMap(self)\n"," |      Return the key-value pairs in this RDD to the master as a dictionary.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This method should only be used if the resulting data is expected\n"," |      to be small, as all the data is loaded into the driver's memory.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n"," |      >>> m[1]\n"," |      2\n"," |      >>> m[3]\n"," |      4\n"," |  \n"," |  collectWithJobGroup(self, groupId, description, interruptOnCancel=False)\n"," |      When collect rdd, use this method to specify job group.\n"," |      \n"," |      .. versionadded:: 3.0.0\n"," |      .. deprecated:: 3.1.0\n"," |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n"," |  \n"," |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash at 0x7f09c77efcb0>)\n"," |      Generic function to combine the elements for each key using a custom\n"," |      set of aggregation functions.\n"," |      \n"," |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n"," |      type\" C.\n"," |      \n"," |      Users provide three functions:\n"," |      \n"," |          - `createCombiner`, which turns a V into a C (e.g., creates\n"," |            a one-element list)\n"," |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n"," |            a list)\n"," |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n"," |            the lists)\n"," |      \n"," |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n"," |      modify and return their first argument instead of creating a new C.\n"," |      \n"," |      In addition, users can control the partitioning of the output RDD.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      V and C can be different -- for example, one might group an RDD of type\n"," |          (Int, Int) into an RDD of type (Int, List[Int]).\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n"," |      >>> def to_list(a):\n"," |      ...     return [a]\n"," |      ...\n"," |      >>> def append(a, b):\n"," |      ...     a.append(b)\n"," |      ...     return a\n"," |      ...\n"," |      >>> def extend(a, b):\n"," |      ...     a.extend(b)\n"," |      ...     return a\n"," |      ...\n"," |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n"," |      [('a', [1, 2]), ('b', [1])]\n"," |  \n"," |  count(self)\n"," |      Return the number of elements in this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([2, 3, 4]).count()\n"," |      3\n"," |  \n"," |  countApprox(self, timeout, confidence=0.95)\n"," |      Approximate version of count() that returns a potentially incomplete\n"," |      result within a timeout, even if not all tasks have finished.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(1000), 10)\n"," |      >>> rdd.countApprox(1000, 1.0)\n"," |      1000\n"," |  \n"," |  countApproxDistinct(self, relativeSD=0.05)\n"," |      Return approximate number of distinct elements in the RDD.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      relativeSD : float, optional\n"," |          Relative accuracy. Smaller values create\n"," |          counters that require more space.\n"," |          It must be greater than 0.000017.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      The algorithm used is based on streamlib's implementation of\n"," |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n"," |      of The Art Cardinality Estimation Algorithm\", available here\n"," |      <https://doi.org/10.1145/2452376.2452456>`_.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n"," |      >>> 900 < n < 1100\n"," |      True\n"," |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n"," |      >>> 16 < n < 24\n"," |      True\n"," |  \n"," |  countByKey(self)\n"," |      Count the number of elements for each key, and return the result to the\n"," |      master as a dictionary.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n"," |      >>> sorted(rdd.countByKey().items())\n"," |      [('a', 2), ('b', 1)]\n"," |  \n"," |  countByValue(self)\n"," |      Return the count of each unique value in this RDD as a dictionary of\n"," |      (value, count) pairs.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n"," |      [(1, 2), (2, 3)]\n"," |  \n"," |  distinct(self, numPartitions=None)\n"," |      Return a new RDD containing the distinct elements in this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n"," |      [1, 2, 3]\n"," |  \n"," |  filter(self, f)\n"," |      Return a new RDD containing only the elements that satisfy a predicate.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n"," |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n"," |      [2, 4]\n"," |  \n"," |  first(self)\n"," |      Return the first element in this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([2, 3, 4]).first()\n"," |      2\n"," |      >>> sc.parallelize([]).first()\n"," |      Traceback (most recent call last):\n"," |          ...\n"," |      ValueError: RDD is empty\n"," |  \n"," |  flatMap(self, f, preservesPartitioning=False)\n"," |      Return a new RDD by first applying a function to all elements of this\n"," |      RDD, and then flattening the results.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([2, 3, 4])\n"," |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n"," |      [1, 1, 1, 2, 2, 3]\n"," |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n"," |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n"," |  \n"," |  flatMapValues(self, f)\n"," |      Pass each value in the key-value pair RDD through a flatMap function\n"," |      without changing the keys; this also retains the original RDD's\n"," |      partitioning.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n"," |      >>> def f(x): return x\n"," |      >>> x.flatMapValues(f).collect()\n"," |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n"," |  \n"," |  fold(self, zeroValue, op)\n"," |      Aggregate the elements of each partition, and then the results for all\n"," |      the partitions, using a given associative function and a neutral \"zero value.\"\n"," |      \n"," |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n"," |      as its result value to avoid object allocation; however, it should not\n"," |      modify ``t2``.\n"," |      \n"," |      This behaves somewhat differently from fold operations implemented\n"," |      for non-distributed collections in functional languages like Scala.\n"," |      This fold operation may be applied to partitions individually, and then\n"," |      fold those results into the final result, rather than apply the fold\n"," |      to each element sequentially in some defined ordering. For functions\n"," |      that are not commutative, the result may differ from that of a fold\n"," |      applied to a non-distributed collection.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> from operator import add\n"," |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n"," |      15\n"," |  \n"," |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f09c77efcb0>)\n"," |      Merge the values for each key using an associative function \"func\"\n"," |      and a neutral \"zeroValue\" which may be added to the result an\n"," |      arbitrary number of times, and must not change the result\n"," |      (e.g., 0 for addition, or 1 for multiplication.).\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n"," |      >>> from operator import add\n"," |      >>> sorted(rdd.foldByKey(0, add).collect())\n"," |      [('a', 2), ('b', 1)]\n"," |  \n"," |  foreach(self, f)\n"," |      Applies a function to all elements of this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> def f(x): print(x)\n"," |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n"," |  \n"," |  foreachPartition(self, f)\n"," |      Applies a function to each partition of this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> def f(iterator):\n"," |      ...     for x in iterator:\n"," |      ...          print(x)\n"," |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n"," |  \n"," |  fullOuterJoin(self, other, numPartitions=None)\n"," |      Perform a right outer join of `self` and `other`.\n"," |      \n"," |      For each element (k, v) in `self`, the resulting RDD will either\n"," |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n"," |      (k, (v, None)) if no elements in `other` have key k.\n"," |      \n"," |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n"," |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n"," |      (k, (None, w)) if no elements in `self` have key k.\n"," |      \n"," |      Hash-partitions the resulting RDD into the given number of partitions.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n"," |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n"," |      >>> sorted(x.fullOuterJoin(y).collect())\n"," |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n"," |  \n"," |  getCheckpointFile(self)\n"," |      Gets the name of the file to which this RDD was checkpointed\n"," |      \n"," |      Not defined if RDD is checkpointed locally.\n"," |  \n"," |  getNumPartitions(self)\n"," |      Returns the number of partitions in RDD\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n"," |      >>> rdd.getNumPartitions()\n"," |      2\n"," |  \n"," |  getResourceProfile(self)\n"," |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n"," |      if it wasn't specified.\n"," |      \n"," |      .. versionadded:: 3.1.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :py:class:`pyspark.resource.ResourceProfile`\n"," |          The the user specified profile or None if none were specified\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This API is experimental\n"," |  \n"," |  getStorageLevel(self)\n"," |      Get the RDD's current storage level.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd1 = sc.parallelize([1,2])\n"," |      >>> rdd1.getStorageLevel()\n"," |      StorageLevel(False, False, False, False, 1)\n"," |      >>> print(rdd1.getStorageLevel())\n"," |      Serialized 1x Replicated\n"," |  \n"," |  glom(self)\n"," |      Return an RDD created by coalescing all elements within each partition\n"," |      into a list.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n"," |      >>> sorted(rdd.glom().collect())\n"," |      [[1, 2], [3, 4]]\n"," |  \n"," |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash at 0x7f09c77efcb0>)\n"," |      Return an RDD of grouped items.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n"," |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n"," |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n"," |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n"," |  \n"," |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f09c77efcb0>)\n"," |      Group the values for each key in the RDD into a single sequence.\n"," |      Hash-partitions the resulting RDD with numPartitions partitions.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      If you are grouping in order to perform an aggregation (such as a\n"," |      sum or average) over each key, using reduceByKey or aggregateByKey will\n"," |      provide much better performance.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n"," |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n"," |      [('a', 2), ('b', 1)]\n"," |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n"," |      [('a', [1, 1]), ('b', [1])]\n"," |  \n"," |  groupWith(self, other, *others)\n"," |      Alias for cogroup but with support for multiple RDDs.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n"," |      >>> y = sc.parallelize([(\"a\", 2)])\n"," |      >>> z = sc.parallelize([(\"b\", 42)])\n"," |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n"," |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n"," |  \n"," |  histogram(self, buckets)\n"," |      Compute a histogram using the provided buckets. The buckets\n"," |      are all open to the right except for the last which is closed.\n"," |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n"," |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n"," |      and 50 we would have a histogram of 1,0,1.\n"," |      \n"," |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n"," |      this can be switched from an O(log n) insertion to O(1) per\n"," |      element (where n is the number of buckets).\n"," |      \n"," |      Buckets must be sorted, not contain any duplicates, and have\n"," |      at least two elements.\n"," |      \n"," |      If `buckets` is a number, it will generate buckets which are\n"," |      evenly spaced between the minimum and maximum of the RDD. For\n"," |      example, if the min value is 0 and the max is 100, given `buckets`\n"," |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n"," |      be at least 1. An exception is raised if the RDD contains infinity.\n"," |      If the elements in the RDD do not vary (max == min), a single bucket\n"," |      will be used.\n"," |      \n"," |      The return value is a tuple of buckets and histogram.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(51))\n"," |      >>> rdd.histogram(2)\n"," |      ([0, 25, 50], [25, 26])\n"," |      >>> rdd.histogram([0, 5, 25, 50])\n"," |      ([0, 5, 25, 50], [5, 20, 26])\n"," |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n"," |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n"," |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n"," |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n"," |      (('a', 'b', 'c'), [2, 2])\n"," |  \n"," |  id(self)\n"," |      A unique ID for this RDD (within its SparkContext).\n"," |  \n"," |  intersection(self, other)\n"," |      Return the intersection of this RDD and another one. The output will\n"," |      not contain any duplicate elements, even if the input RDDs did.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This method performs a shuffle internally.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n"," |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n"," |      >>> rdd1.intersection(rdd2).collect()\n"," |      [1, 2, 3]\n"," |  \n"," |  isCheckpointed(self)\n"," |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n"," |  \n"," |  isEmpty(self)\n"," |      Returns true if and only if the RDD contains no elements at all.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      An RDD may be empty even when it has at least 1 partition.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([]).isEmpty()\n"," |      True\n"," |      >>> sc.parallelize([1]).isEmpty()\n"," |      False\n"," |  \n"," |  isLocallyCheckpointed(self)\n"," |      Return whether this RDD is marked for local checkpointing.\n"," |      \n"," |      Exposed for testing.\n"," |  \n"," |  join(self, other, numPartitions=None)\n"," |      Return an RDD containing all pairs of elements with matching keys in\n"," |      `self` and `other`.\n"," |      \n"," |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n"," |      (k, v1) is in `self` and (k, v2) is in `other`.\n"," |      \n"," |      Performs a hash join across the cluster.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n"," |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n"," |      >>> sorted(x.join(y).collect())\n"," |      [('a', (1, 2)), ('a', (1, 3))]\n"," |  \n"," |  keyBy(self, f)\n"," |      Creates tuples of the elements in this RDD by applying `f`.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n"," |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n"," |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n"," |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n"," |  \n"," |  keys(self)\n"," |      Return an RDD with the keys of each tuple.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n"," |      >>> m.collect()\n"," |      [1, 3]\n"," |  \n"," |  leftOuterJoin(self, other, numPartitions=None)\n"," |      Perform a left outer join of `self` and `other`.\n"," |      \n"," |      For each element (k, v) in `self`, the resulting RDD will either\n"," |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n"," |      (k, (v, None)) if no elements in `other` have key k.\n"," |      \n"," |      Hash-partitions the resulting RDD into the given number of partitions.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n"," |      >>> y = sc.parallelize([(\"a\", 2)])\n"," |      >>> sorted(x.leftOuterJoin(y).collect())\n"," |      [('a', (1, 2)), ('b', (4, None))]\n"," |  \n"," |  localCheckpoint(self)\n"," |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n"," |      \n"," |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n"," |      step of replicating the materialized data in a reliable distributed file system. This is\n"," |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n"," |      \n"," |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n"," |      data is written to ephemeral local storage in the executors instead of to a reliable,\n"," |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n"," |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n"," |      \n"," |      This is NOT safe to use with dynamic allocation, which removes executors along\n"," |      with their cached blocks. If you must use both features, you are advised to set\n"," |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n"," |      \n"," |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n"," |  \n"," |  lookup(self, key)\n"," |      Return the list of values in the RDD for key `key`. This operation\n"," |      is done efficiently if the RDD has a known partitioner by only\n"," |      searching the partition that the key maps to.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> l = range(1000)\n"," |      >>> rdd = sc.parallelize(zip(l, l), 10)\n"," |      >>> rdd.lookup(42)  # slow\n"," |      [42]\n"," |      >>> sorted = rdd.sortByKey()\n"," |      >>> sorted.lookup(42)  # fast\n"," |      [42]\n"," |      >>> sorted.lookup(1024)\n"," |      []\n"," |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n"," |      >>> list(rdd2.lookup(('a', 'b'))[0])\n"," |      ['c']\n"," |  \n"," |  map(self, f, preservesPartitioning=False)\n"," |      Return a new RDD by applying a function to each element of this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n"," |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n"," |      [('a', 1), ('b', 1), ('c', 1)]\n"," |  \n"," |  mapPartitions(self, f, preservesPartitioning=False)\n"," |      Return a new RDD by applying a function to each partition of this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n"," |      >>> def f(iterator): yield sum(iterator)\n"," |      >>> rdd.mapPartitions(f).collect()\n"," |      [3, 7]\n"," |  \n"," |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n"," |      Return a new RDD by applying a function to each partition of this RDD,\n"," |      while tracking the index of the original partition.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n"," |      >>> def f(splitIndex, iterator): yield splitIndex\n"," |      >>> rdd.mapPartitionsWithIndex(f).sum()\n"," |      6\n"," |  \n"," |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n"," |      Return a new RDD by applying a function to each partition of this RDD,\n"," |      while tracking the index of the original partition.\n"," |      \n"," |      .. deprecated:: 0.9.0\n"," |          use :py:meth:`RDD.mapPartitionsWithIndex` instead.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n"," |      >>> def f(splitIndex, iterator): yield splitIndex\n"," |      >>> rdd.mapPartitionsWithSplit(f).sum()\n"," |      6\n"," |  \n"," |  mapValues(self, f)\n"," |      Pass each value in the key-value pair RDD through a map function\n"," |      without changing the keys; this also retains the original RDD's\n"," |      partitioning.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n"," |      >>> def f(x): return len(x)\n"," |      >>> x.mapValues(f).collect()\n"," |      [('a', 3), ('b', 1)]\n"," |  \n"," |  max(self, key=None)\n"," |      Find the maximum item in this RDD.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      key : function, optional\n"," |          A function used to generate key for comparing\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n"," |      >>> rdd.max()\n"," |      43.0\n"," |      >>> rdd.max(key=str)\n"," |      5.0\n"," |  \n"," |  mean(self)\n"," |      Compute the mean of this RDD's elements.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([1, 2, 3]).mean()\n"," |      2.0\n"," |  \n"," |  meanApprox(self, timeout, confidence=0.95)\n"," |      Approximate operation to return the mean within a timeout\n"," |      or meet the confidence.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(1000), 10)\n"," |      >>> r = sum(range(1000)) / 1000.0\n"," |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n"," |      True\n"," |  \n"," |  min(self, key=None)\n"," |      Find the minimum item in this RDD.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      key : function, optional\n"," |          A function used to generate key for comparing\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n"," |      >>> rdd.min()\n"," |      2.0\n"," |      >>> rdd.min(key=str)\n"," |      10.0\n"," |  \n"," |  name(self)\n"," |      Return the name of this RDD.\n"," |  \n"," |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash at 0x7f09c77efcb0>)\n"," |      Return a copy of the RDD partitioned using the specified partitioner.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n"," |      >>> sets = pairs.partitionBy(2).glom().collect()\n"," |      >>> len(set(sets[0]).intersection(set(sets[1])))\n"," |      0\n"," |  \n"," |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n"," |      Set this RDD's storage level to persist its values across operations\n"," |      after the first time it is computed. This can only be used to assign\n"," |      a new storage level if the RDD does not have a storage level set yet.\n"," |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n"," |      >>> rdd.persist().is_cached\n"," |      True\n"," |  \n"," |  pipe(self, command, env=None, checkCode=False)\n"," |      Return an RDD created by piping elements to a forked external process.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      command : str\n"," |          command to run.\n"," |      env : dict, optional\n"," |          environment variables to set.\n"," |      checkCode : bool, optional\n"," |          whether or not to check the return value of the shell command.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n"," |      ['1', '2', '', '3']\n"," |  \n"," |  randomSplit(self, weights, seed=None)\n"," |      Randomly splits this RDD with the provided weights.\n"," |      \n"," |      weights : list\n"," |          weights for splits, will be normalized if they don't sum to 1\n"," |      seed : int, optional\n"," |          random seed\n"," |      \n"," |      Returns\n"," |      -------\n"," |      list\n"," |          split RDDs in a list\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(500), 1)\n"," |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n"," |      >>> len(rdd1.collect() + rdd2.collect())\n"," |      500\n"," |      >>> 150 < rdd1.count() < 250\n"," |      True\n"," |      >>> 250 < rdd2.count() < 350\n"," |      True\n"," |  \n"," |  reduce(self, f)\n"," |      Reduces the elements of this RDD using the specified commutative and\n"," |      associative binary operator. Currently reduces partitions locally.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> from operator import add\n"," |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n"," |      15\n"," |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n"," |      10\n"," |      >>> sc.parallelize([]).reduce(add)\n"," |      Traceback (most recent call last):\n"," |          ...\n"," |      ValueError: Can not reduce() empty RDD\n"," |  \n"," |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f09c77efcb0>)\n"," |      Merge the values for each key using an associative and commutative reduce function.\n"," |      \n"," |      This will also perform the merging locally on each mapper before\n"," |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n"," |      \n"," |      Output will be partitioned with `numPartitions` partitions, or\n"," |      the default parallelism level if `numPartitions` is not specified.\n"," |      Default partitioner is hash-partition.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> from operator import add\n"," |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n"," |      >>> sorted(rdd.reduceByKey(add).collect())\n"," |      [('a', 2), ('b', 1)]\n"," |  \n"," |  reduceByKeyLocally(self, func)\n"," |      Merge the values for each key using an associative and commutative reduce function, but\n"," |      return the results immediately to the master as a dictionary.\n"," |      \n"," |      This will also perform the merging locally on each mapper before\n"," |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> from operator import add\n"," |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n"," |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n"," |      [('a', 2), ('b', 1)]\n"," |  \n"," |  repartition(self, numPartitions)\n"," |       Return a new RDD that has exactly numPartitions partitions.\n"," |      \n"," |       Can increase or decrease the level of parallelism in this RDD.\n"," |       Internally, this uses a shuffle to redistribute data.\n"," |       If you are decreasing the number of partitions in this RDD, consider\n"," |       using `coalesce`, which can avoid performing a shuffle.\n"," |      \n"," |      Examples\n"," |      --------\n"," |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n"," |       >>> sorted(rdd.glom().collect())\n"," |       [[1], [2, 3], [4, 5], [6, 7]]\n"," |       >>> len(rdd.repartition(2).glom().collect())\n"," |       2\n"," |       >>> len(rdd.repartition(10).glom().collect())\n"," |       10\n"," |  \n"," |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f09c77efcb0>, ascending=True, keyfunc=<function RDD.<lambda> at 0x7f09a1733dd0>)\n"," |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n"," |      sort records by their keys.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n"," |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n"," |      >>> rdd2.glom().collect()\n"," |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n"," |  \n"," |  rightOuterJoin(self, other, numPartitions=None)\n"," |      Perform a right outer join of `self` and `other`.\n"," |      \n"," |      For each element (k, w) in `other`, the resulting RDD will either\n"," |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n"," |      if no elements in `self` have key k.\n"," |      \n"," |      Hash-partitions the resulting RDD into the given number of partitions.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n"," |      >>> y = sc.parallelize([(\"a\", 2)])\n"," |      >>> sorted(y.rightOuterJoin(x).collect())\n"," |      [('a', (2, 1)), ('b', (None, 4))]\n"," |  \n"," |  sample(self, withReplacement, fraction, seed=None)\n"," |      Return a sampled subset of this RDD.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      withReplacement : bool\n"," |          can elements be sampled multiple times (replaced when sampled out)\n"," |      fraction : float\n"," |          expected size of the sample as a fraction of this RDD's size\n"," |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n"," |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n"," |      seed : int, optional\n"," |          seed for the random number generator\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This is not guaranteed to provide exactly the fraction specified of the total\n"," |      count of the given :class:`DataFrame`.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(100), 4)\n"," |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n"," |      True\n"," |  \n"," |  sampleByKey(self, withReplacement, fractions, seed=None)\n"," |      Return a subset of this RDD sampled by key (via stratified sampling).\n"," |      Create a sample of this RDD using variable sampling rates for\n"," |      different keys as specified by fractions, a key to sampling rate map.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n"," |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n"," |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n"," |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n"," |      True\n"," |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n"," |      True\n"," |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n"," |      True\n"," |  \n"," |  sampleStdev(self)\n"," |      Compute the sample standard deviation of this RDD's elements (which\n"," |      corrects for bias in estimating the standard deviation by dividing by\n"," |      N-1 instead of N).\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n"," |      1.0\n"," |  \n"," |  sampleVariance(self)\n"," |      Compute the sample variance of this RDD's elements (which corrects\n"," |      for bias in estimating the variance by dividing by N-1 instead of N).\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n"," |      1.0\n"," |  \n"," |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n"," |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n"," |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n"," |      converted for output using either user specified converters or, by default,\n"," |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      conf : dict\n"," |          Hadoop job configuration\n"," |      keyConverter : str, optional\n"," |          fully qualified classname of key converter (None by default)\n"," |      valueConverter : str, optional\n"," |          fully qualified classname of value converter (None by default)\n"," |  \n"," |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n"," |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n"," |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n"," |      will be inferred if not specified. Keys and values are converted for output using either\n"," |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n"," |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n"," |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      path : str\n"," |          path to Hadoop file\n"," |      outputFormatClass : str\n"," |          fully qualified classname of Hadoop OutputFormat\n"," |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n"," |      keyClass : str, optional\n"," |          fully qualified classname of key Writable class\n"," |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n"," |      valueClass : str, optional\n"," |          fully qualified classname of value Writable class\n"," |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n"," |      keyConverter : str, optional\n"," |          fully qualified classname of key converter (None by default)\n"," |      valueConverter : str, optional\n"," |          fully qualified classname of value converter (None by default)\n"," |      conf : dict, optional\n"," |          (None by default)\n"," |      compressionCodecClass : str\n"," |          fully qualified classname of the compression codec class\n"," |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n"," |  \n"," |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n"," |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n"," |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n"," |      converted for output using either user specified converters or, by default,\n"," |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      conf : dict\n"," |          Hadoop job configuration\n"," |      keyConverter : str, optional\n"," |          fully qualified classname of key converter (None by default)\n"," |      valueConverter : str, optional\n"," |          fully qualified classname of value converter (None by default)\n"," |  \n"," |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n"," |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n"," |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n"," |      will be inferred if not specified. Keys and values are converted for output using either\n"," |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n"," |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n"," |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n"," |      \n"," |      path : str\n"," |          path to Hadoop file\n"," |      outputFormatClass : str\n"," |          fully qualified classname of Hadoop OutputFormat\n"," |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n"," |      keyClass : str, optional\n"," |          fully qualified classname of key Writable class\n"," |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n"," |      valueClass : str, optional\n"," |          fully qualified classname of value Writable class\n"," |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n"," |      keyConverter : str, optional\n"," |          fully qualified classname of key converter (None by default)\n"," |      valueConverter : str, optional\n"," |          fully qualified classname of value converter (None by default)\n"," |      conf : dict, optional\n"," |          Hadoop job configuration (None by default)\n"," |  \n"," |  saveAsPickleFile(self, path, batchSize=10)\n"," |      Save this RDD as a SequenceFile of serialized objects. The serializer\n"," |      used is :class:`pyspark.serializers.PickleSerializer`, default batch size\n"," |      is 10.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> from tempfile import NamedTemporaryFile\n"," |      >>> tmpFile = NamedTemporaryFile(delete=True)\n"," |      >>> tmpFile.close()\n"," |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n"," |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n"," |      ['1', '2', 'rdd', 'spark']\n"," |  \n"," |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n"," |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n"," |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n"," |      RDD's key and value types. The mechanism is as follows:\n"," |      \n"," |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n"," |          2. Keys and values of this Java RDD are converted to Writables and written out.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      path : str\n"," |          path to sequence file\n"," |      compressionCodecClass : str, optional\n"," |          fully qualified classname of the compression codec class\n"," |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n"," |  \n"," |  saveAsTextFile(self, path, compressionCodecClass=None)\n"," |      Save this RDD as a text file, using string representations of elements.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      path : str\n"," |          path to text file\n"," |      compressionCodecClass : str, optional\n"," |          fully qualified classname of the compression codec class\n"," |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> from tempfile import NamedTemporaryFile\n"," |      >>> tempFile = NamedTemporaryFile(delete=True)\n"," |      >>> tempFile.close()\n"," |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n"," |      >>> from fileinput import input\n"," |      >>> from glob import glob\n"," |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n"," |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n"," |      \n"," |      Empty lines are tolerated when saving to text files.\n"," |      \n"," |      >>> from tempfile import NamedTemporaryFile\n"," |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n"," |      >>> tempFile2.close()\n"," |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n"," |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n"," |      '\\n\\n\\nbar\\nfoo\\n'\n"," |      \n"," |      Using compressionCodecClass\n"," |      \n"," |      >>> from tempfile import NamedTemporaryFile\n"," |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n"," |      >>> tempFile3.close()\n"," |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n"," |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n"," |      >>> from fileinput import input, hook_compressed\n"," |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n"," |      >>> b''.join(result).decode('utf-8')\n"," |      'bar\\nfoo\\n'\n"," |  \n"," |  setName(self, name)\n"," |      Assign a name to this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd1 = sc.parallelize([1, 2])\n"," |      >>> rdd1.setName('RDD1').name()\n"," |      'RDD1'\n"," |  \n"," |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n"," |      Sorts this RDD by the given keyfunc\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n"," |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n"," |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n"," |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n"," |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n"," |  \n"," |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda> at 0x7f09a1733ef0>)\n"," |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n"," |      >>> sc.parallelize(tmp).sortByKey().first()\n"," |      ('1', 3)\n"," |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n"," |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n"," |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n"," |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n"," |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n"," |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n"," |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n"," |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n"," |  \n"," |  stats(self)\n"," |      Return a :class:`StatCounter` object that captures the mean, variance\n"," |      and count of the RDD's elements in one operation.\n"," |  \n"," |  stdev(self)\n"," |      Compute the standard deviation of this RDD's elements.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([1, 2, 3]).stdev()\n"," |      0.816...\n"," |  \n"," |  subtract(self, other, numPartitions=None)\n"," |      Return each value in `self` that is not contained in `other`.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n"," |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n"," |      >>> sorted(x.subtract(y).collect())\n"," |      [('a', 1), ('b', 4), ('b', 5)]\n"," |  \n"," |  subtractByKey(self, other, numPartitions=None)\n"," |      Return each (key, value) pair in `self` that has no pair with matching\n"," |      key in `other`.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n"," |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n"," |      >>> sorted(x.subtractByKey(y).collect())\n"," |      [('b', 4), ('b', 5)]\n"," |  \n"," |  sum(self)\n"," |      Add up the elements in this RDD.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n"," |      6.0\n"," |  \n"," |  sumApprox(self, timeout, confidence=0.95)\n"," |      Approximate operation to return the sum within a timeout\n"," |      or meet the confidence.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(1000), 10)\n"," |      >>> r = sum(range(1000))\n"," |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n"," |      True\n"," |  \n"," |  take(self, num)\n"," |      Take the first num elements of the RDD.\n"," |      \n"," |      It works by first scanning one partition, and use the results from\n"," |      that partition to estimate the number of additional partitions needed\n"," |      to satisfy the limit.\n"," |      \n"," |      Translated from the Scala implementation in RDD#take().\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This method should only be used if the resulting array is expected\n"," |      to be small, as all the data is loaded into the driver's memory.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n"," |      [2, 3]\n"," |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n"," |      [2, 3, 4, 5, 6]\n"," |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n"," |      [91, 92, 93]\n"," |  \n"," |  takeOrdered(self, num, key=None)\n"," |      Get the N elements from an RDD ordered in ascending order or as\n"," |      specified by the optional key function.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This method should only be used if the resulting array is expected\n"," |      to be small, as all the data is loaded into the driver's memory.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n"," |      [1, 2, 3, 4, 5, 6]\n"," |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n"," |      [10, 9, 7, 6, 5, 4]\n"," |  \n"," |  takeSample(self, withReplacement, num, seed=None)\n"," |      Return a fixed-size sampled subset of this RDD.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This method should only be used if the resulting array is expected\n"," |      to be small, as all the data is loaded into the driver's memory.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(0, 10))\n"," |      >>> len(rdd.takeSample(True, 20, 1))\n"," |      20\n"," |      >>> len(rdd.takeSample(False, 5, 2))\n"," |      5\n"," |      >>> len(rdd.takeSample(False, 15, 3))\n"," |      10\n"," |  \n"," |  toDebugString(self)\n"," |      A description of this RDD and its recursive dependencies for debugging.\n"," |  \n"," |  toLocalIterator(self, prefetchPartitions=False)\n"," |      Return an iterator that contains all of the elements in this RDD.\n"," |      The iterator will consume as much memory as the largest partition in this RDD.\n"," |      With prefetch it may consume up to the memory of the 2 largest partitions.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      prefetchPartitions : bool, optional\n"," |          If Spark should pre-fetch the next partition\n"," |          before it is needed.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize(range(10))\n"," |      >>> [x for x in rdd.toLocalIterator()]\n"," |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"," |  \n"," |  top(self, num, key=None)\n"," |      Get the top N elements from an RDD.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This method should only be used if the resulting array is expected\n"," |      to be small, as all the data is loaded into the driver's memory.\n"," |      \n"," |      It returns the list sorted in descending order.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n"," |      [12]\n"," |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n"," |      [6, 5]\n"," |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n"," |      [4, 3, 2]\n"," |  \n"," |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n"," |      Aggregates the elements of this RDD in a multi-level tree\n"," |      pattern.\n"," |      \n"," |      depth : int, optional\n"," |          suggested depth of the tree (default: 2)\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> add = lambda x, y: x + y\n"," |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n"," |      >>> rdd.treeAggregate(0, add, add)\n"," |      -5\n"," |      >>> rdd.treeAggregate(0, add, add, 1)\n"," |      -5\n"," |      >>> rdd.treeAggregate(0, add, add, 2)\n"," |      -5\n"," |      >>> rdd.treeAggregate(0, add, add, 5)\n"," |      -5\n"," |      >>> rdd.treeAggregate(0, add, add, 10)\n"," |      -5\n"," |  \n"," |  treeReduce(self, f, depth=2)\n"," |      Reduces the elements of this RDD in a multi-level tree pattern.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      f : function\n"," |      depth : int, optional\n"," |          suggested depth of the tree (default: 2)\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> add = lambda x, y: x + y\n"," |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n"," |      >>> rdd.treeReduce(add)\n"," |      -5\n"," |      >>> rdd.treeReduce(add, 1)\n"," |      -5\n"," |      >>> rdd.treeReduce(add, 2)\n"," |      -5\n"," |      >>> rdd.treeReduce(add, 5)\n"," |      -5\n"," |      >>> rdd.treeReduce(add, 10)\n"," |      -5\n"," |  \n"," |  union(self, other)\n"," |      Return the union of this RDD and another one.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n"," |      >>> rdd.union(rdd).collect()\n"," |      [1, 1, 2, 3, 1, 1, 2, 3]\n"," |  \n"," |  unpersist(self, blocking=False)\n"," |      Mark the RDD as non-persistent, and remove all blocks for it from\n"," |      memory and disk.\n"," |      \n"," |      .. versionchanged:: 3.0.0\n"," |         Added optional argument `blocking` to specify whether to block until all\n"," |         blocks are deleted.\n"," |  \n"," |  values(self)\n"," |      Return an RDD with the values of each tuple.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n"," |      >>> m.collect()\n"," |      [2, 4]\n"," |  \n"," |  variance(self)\n"," |      Compute the variance of this RDD's elements.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([1, 2, 3]).variance()\n"," |      0.666...\n"," |  \n"," |  withResources(self, profile)\n"," |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n"," |      This is only supported on certain cluster managers and currently requires dynamic\n"," |      allocation to be enabled. It will result in new executors with the resources specified\n"," |      being acquired to calculate the RDD.\n"," |      \n"," |      .. versionadded:: 3.1.0\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This API is experimental\n"," |  \n"," |  zip(self, other)\n"," |      Zips this RDD with another one, returning key-value pairs with the\n"," |      first element in each RDD second element in each RDD, etc. Assumes\n"," |      that the two RDDs have the same number of partitions and the same\n"," |      number of elements in each partition (e.g. one was made through\n"," |      a map on the other).\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> x = sc.parallelize(range(0,5))\n"," |      >>> y = sc.parallelize(range(1000, 1005))\n"," |      >>> x.zip(y).collect()\n"," |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n"," |  \n"," |  zipWithIndex(self)\n"," |      Zips this RDD with its element indices.\n"," |      \n"," |      The ordering is first based on the partition index and then the\n"," |      ordering of items within each partition. So the first item in\n"," |      the first partition gets index 0, and the last item in the last\n"," |      partition receives the largest index.\n"," |      \n"," |      This method needs to trigger a spark job when this RDD contains\n"," |      more than one partitions.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n"," |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n"," |  \n"," |  zipWithUniqueId(self)\n"," |      Zips this RDD with generated unique Long ids.\n"," |      \n"," |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n"," |      n is the number of partitions. So there may exist gaps, but this\n"," |      method won't trigger a spark job, which is different from\n"," |      :meth:`zipWithIndex`.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n"," |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors defined here:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  context\n"," |      The :class:`SparkContext` that this RDD was created on.\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7UUd0uvtlfUN","executionInfo":{"status":"ok","timestamp":1636390153466,"user_tz":-60,"elapsed":18,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"fb768738-415f-42cf-f797-c7bc1e9dc6db"},"source":["type(bible_file)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.rdd.RDD"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FH5HxGtlgV7","executionInfo":{"status":"ok","timestamp":1636390153467,"user_tz":-60,"elapsed":4,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"dbeb4552-0dd6-4ccd-e6ce-38eff9f34a31"},"source":["print(bible_file.__doc__)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n","    Represents an immutable, partitioned collection of elements that can be\n","    operated on in parallel.\n","    \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"id":"osJTcSgEloEF","executionInfo":{"status":"ok","timestamp":1636390154523,"user_tz":-60,"elapsed":1058,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"bc8f856b-e42c-484f-c772-2a8abcaca7c6"},"source":["bible_file.first()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. '"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"obutHVAosM44"},"source":["# Word Count"]},{"cell_type":"markdown","metadata":{"id":"TLK85mUYsPRb"},"source":["## Map & Reduce\n"]},{"cell_type":"code","metadata":{"id":"ZIoP9mYjagiT","executionInfo":{"status":"ok","timestamp":1636390154523,"user_tz":-60,"elapsed":3,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["first_line = \"In the beginning God created the heaven and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.\""],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZKyzXXXarq8","executionInfo":{"status":"ok","timestamp":1636390154835,"user_tz":-60,"elapsed":7,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["def decompose_to_words(line):\n","  return line.split()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"EiMmT73EnV-L","executionInfo":{"status":"ok","timestamp":1636390154836,"user_tz":-60,"elapsed":8,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["bible_words = bible_file.flatMap(lambda line : line.split(\" \"))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kYC-YCfouB3","executionInfo":{"status":"ok","timestamp":1636390154836,"user_tz":-60,"elapsed":7,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["word_counts = bible_words.map(lambda word : (word, 1))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMr7vTwzpCBR","executionInfo":{"status":"ok","timestamp":1636390154836,"user_tz":-60,"elapsed":7,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["word_counts = word_counts.reduceByKey(lambda cumulVal, newVal : cumulVal + newVal)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJUIUrHToGQ2","executionInfo":{"status":"ok","timestamp":1636390154837,"user_tz":-60,"elapsed":8,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"880cd061-3549-4a1e-9e1b-7ba37e88816c"},"source":["type(bible_words)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.rdd.PipelinedRDD"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"oKmzH87D_e9B"},"source":["## Lazy Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I52c0UCJqst8","executionInfo":{"status":"ok","timestamp":1636390157179,"user_tz":-60,"elapsed":2347,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"afabaa21-1599-4db1-e1db-126fa325d73b"},"source":["word_counts.take(4)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('God', 2186), ('created', 36), ('And', 12163), ('earth', 326)]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"zLA613A3qDVj","executionInfo":{"status":"ok","timestamp":1636390157510,"user_tz":-60,"elapsed":347,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["word_counts = word_counts.sortBy(lambda kv_pair : kv_pair[1], ascending=False)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KR5WlK3IqZdu","executionInfo":{"status":"ok","timestamp":1636390158048,"user_tz":-60,"elapsed":541,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"20ae8b3e-ca74-40df-8af8-45576f194c1b"},"source":["word_counts.take(4)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('the', 59835), ('and', 37322), ('of', 32972), ('', 30383)]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"KKsvEzwRrGrn","executionInfo":{"status":"ok","timestamp":1636390158638,"user_tz":-60,"elapsed":592,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["word_counts.saveAsTextFile(os.path.join(data_dir, 'bible_word_count'))"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfezJ5TmsRkr"},"source":["## CountByValue"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZLH5FPvsZU8","executionInfo":{"status":"ok","timestamp":1636390159404,"user_tz":-60,"elapsed":768,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}},"outputId":"1a432af2-31d7-4c1a-ca59-596f29872d2c"},"source":["bible_file.flatMap(lambda line : line.split(\" \")).countByValue()"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(int,\n","            {'In': 309,\n","             'the': 59835,\n","             'beginning': 68,\n","             'God': 2186,\n","             'created': 36,\n","             'heaven': 202,\n","             'and': 37322,\n","             'earth.': 191,\n","             'And': 12163,\n","             'earth': 326,\n","             'was': 4249,\n","             'without': 372,\n","             'form,': 3,\n","             'void;': 3,\n","             'darkness': 61,\n","             'upon': 2659,\n","             'face': 265,\n","             'of': 32972,\n","             'deep.': 5,\n","             'Spirit': 118,\n","             'moved': 45,\n","             'waters.': 25,\n","             '': 30383,\n","             'said,': 1556,\n","             'Let': 417,\n","             'there': 1799,\n","             'be': 6643,\n","             'light:': 10,\n","             'light.': 28,\n","             'saw': 508,\n","             'light,': 53,\n","             'that': 12107,\n","             'it': 4336,\n","             'good:': 33,\n","             'divided': 54,\n","             'light': 160,\n","             'from': 3471,\n","             'darkness.': 25,\n","             'called': 556,\n","             'Day,': 1,\n","             'he': 9048,\n","             'Night.': 1,\n","             'evening': 31,\n","             'morning': 92,\n","             'were': 2638,\n","             'first': 357,\n","             'day.': 194,\n","             'a': 7763,\n","             'firmament': 14,\n","             'in': 11748,\n","             'midst': 347,\n","             'waters,': 43,\n","             'let': 1052,\n","             'divide': 47,\n","             'waters': 182,\n","             'made': 1270,\n","             'firmament,': 1,\n","             'which': 4119,\n","             'under': 375,\n","             'above': 177,\n","             'firmament:': 1,\n","             'so.': 30,\n","             'Heaven.': 1,\n","             'second': 131,\n","             'gathered': 248,\n","             'together': 237,\n","             'unto': 8744,\n","             'one': 1674,\n","             'place,': 146,\n","             'dry': 59,\n","             'land': 1108,\n","             'appear:': 1,\n","             'Earth;': 1,\n","             'gathering': 11,\n","             'Seas:': 1,\n","             'good.': 54,\n","             'bring': 668,\n","             'forth': 720,\n","             'grass,': 12,\n","             'herb': 13,\n","             'yielding': 7,\n","             'seed,': 49,\n","             'fruit': 155,\n","             'tree': 93,\n","             'after': 1053,\n","             'his': 8032,\n","             'kind,': 23,\n","             'whose': 277,\n","             'seed': 199,\n","             'is': 6600,\n","             'itself,': 16,\n","             'earth:': 104,\n","             'brought': 814,\n","             'fruit,': 22,\n","             'kind:': 6,\n","             'third': 155,\n","             'lights': 6,\n","             'to': 12837,\n","             'day': 1044,\n","             'night;': 19,\n","             'them': 3419,\n","             'for': 7007,\n","             'signs,': 9,\n","             'seasons,': 5,\n","             'days,': 135,\n","             'years:': 28,\n","             'give': 767,\n","             'two': 742,\n","             'great': 835,\n","             'lights;': 1,\n","             'greater': 67,\n","             'rule': 56,\n","             'day,': 315,\n","             'lesser': 3,\n","             'night:': 25,\n","             'stars': 36,\n","             'also.': 53,\n","             'set': 651,\n","             'earth,': 254,\n","             'over': 920,\n","             'night,': 93,\n","             'darkness:': 9,\n","             'fourth': 68,\n","             'abundantly': 17,\n","             'moving': 5,\n","             'creature': 23,\n","             'hath': 2176,\n","             'life,': 90,\n","             'fowl': 19,\n","             'may': 1002,\n","             'fly': 20,\n","             'open': 94,\n","             'heaven.': 77,\n","             'whales,': 1,\n","             'every': 1150,\n","             'living': 109,\n","             'moveth,': 2,\n","             'abundantly,': 8,\n","             'their': 3801,\n","             'winged': 2,\n","             'blessed': 152,\n","             'them,': 1549,\n","             'saying,': 1225,\n","             'Be': 133,\n","             'fruitful,': 11,\n","             'multiply,': 7,\n","             'fill': 42,\n","             'seas,': 11,\n","             'multiply': 36,\n","             'fifth': 50,\n","             'cattle,': 51,\n","             'creeping': 29,\n","             'thing,': 83,\n","             'beast': 90,\n","             'cattle': 71,\n","             'thing': 356,\n","             'creepeth': 13,\n","             'us': 782,\n","             'make': 991,\n","             'man': 1901,\n","             'our': 1127,\n","             'image,': 30,\n","             'likeness:': 2,\n","             'have': 3761,\n","             'dominion': 40,\n","             'fish': 26,\n","             'sea,': 142,\n","             'air,': 13,\n","             'all': 5051,\n","             'So': 574,\n","             'own': 555,\n","             'image': 48,\n","             'him;': 207,\n","             'male': 40,\n","             'female': 11,\n","             'them.': 674,\n","             'said': 2180,\n","             'replenish': 2,\n","             'subdue': 8,\n","             'it:': 168,\n","             'moveth': 6,\n","             'Behold,': 501,\n","             'I': 8566,\n","             'given': 476,\n","             'you': 1149,\n","             'bearing': 19,\n","             'tree,': 62,\n","             'seed;': 6,\n","             'shall': 9658,\n","             'meat.': 20,\n","             'wherein': 147,\n","             'green': 34,\n","             'meat:': 8,\n","             'had': 1899,\n","             'made,': 31,\n","             'and,': 259,\n","             'behold,': 572,\n","             'very': 254,\n","             'sixth': 39,\n","             'Thus': 421,\n","             'heavens': 64,\n","             'finished,': 5,\n","             'host': 105,\n","             'on': 1849,\n","             'seventh': 106,\n","             'ended': 8,\n","             'work': 288,\n","             'made;': 10,\n","             'rested': 18,\n","             'made.': 16,\n","             'sanctified': 53,\n","             'because': 985,\n","             'These': 226,\n","             'are': 2837,\n","             'generations': 34,\n","             'when': 2393,\n","             'they': 6665,\n","             'created,': 4,\n","             'LORD': 3795,\n","             'heavens,': 38,\n","             'plant': 32,\n","             'field': 110,\n","             'before': 1701,\n","             'grew:': 2,\n","             'not': 5681,\n","             'caused': 93,\n","             'rain': 57,\n","             'till': 154,\n","             'ground.': 38,\n","             'But': 1501,\n","             'went': 1240,\n","             'up': 1835,\n","             'mist': 3,\n","             'watered': 10,\n","             'whole': 225,\n","             'formed': 29,\n","             'dust': 60,\n","             'ground,': 53,\n","             'breathed': 2,\n","             'into': 1940,\n","             'nostrils': 6,\n","             'breath': 34,\n","             'life;': 32,\n","             'became': 103,\n","             'soul.': 53,\n","             'planted': 30,\n","             'garden': 25,\n","             'eastward': 12,\n","             'Eden;': 2,\n","             'put': 851,\n","             'whom': 700,\n","             'formed.': 1,\n","             'out': 2378,\n","             'ground': 55,\n","             'grow': 28,\n","             'pleasant': 50,\n","             'sight,': 58,\n","             'good': 494,\n","             'food;': 1,\n","             'life': 193,\n","             'also': 1404,\n","             'garden,': 11,\n","             'knowledge': 90,\n","             'evil.': 48,\n","             'river': 95,\n","             'Eden': 8,\n","             'water': 166,\n","             'garden;': 5,\n","             'thence': 75,\n","             'parted,': 2,\n","             'four': 291,\n","             'heads.': 10,\n","             'The': 1819,\n","             'name': 633,\n","             'Pison:': 1,\n","             'compasseth': 5,\n","             'Havilah,': 5,\n","             'where': 314,\n","             'gold;': 23,\n","             'gold': 138,\n","             'bdellium': 1,\n","             'onyx': 6,\n","             'stone.': 17,\n","             'Gihon:': 3,\n","             'same': 289,\n","             'Ethiopia.': 2,\n","             'Hiddekel:': 1,\n","             'goeth': 124,\n","             'toward': 344,\n","             'east': 101,\n","             'Assyria.': 13,\n","             'Euphrates.': 7,\n","             'took': 688,\n","             'man,': 414,\n","             'him': 3030,\n","             'dress': 9,\n","             'keep': 326,\n","             'it.': 416,\n","             'commanded': 366,\n","             'Of': 179,\n","             'thou': 4537,\n","             'mayest': 111,\n","             'freely': 11,\n","             'eat:': 23,\n","             'evil,': 78,\n","             'shalt': 1586,\n","             'eat': 469,\n","             'eatest': 3,\n","             'thereof': 319,\n","             'surely': 186,\n","             'die.': 66,\n","             'It': 227,\n","             'should': 757,\n","             'alone;': 17,\n","             'will': 3694,\n","             'an': 1652,\n","             'help': 93,\n","             'meet': 113,\n","             'him.': 792,\n","             'field,': 95,\n","             'air;': 4,\n","             'Adam': 20,\n","             'see': 453,\n","             'what': 638,\n","             'would': 423,\n","             'call': 168,\n","             'them:': 299,\n","             'whatsoever': 135,\n","             'creature,': 2,\n","             'thereof.': 128,\n","             'gave': 451,\n","             'names': 64,\n","             'field;': 24,\n","             'but': 2363,\n","             'found': 339,\n","             'deep': 43,\n","             'sleep': 36,\n","             'fall': 188,\n","             'Adam,': 9,\n","             'slept:': 2,\n","             'ribs,': 1,\n","             'closed': 8,\n","             'flesh': 217,\n","             'instead': 34,\n","             'thereof;': 59,\n","             'rib,': 4,\n","             'taken': 277,\n","             'woman,': 68,\n","             'her': 1476,\n","             'man.': 92,\n","             'This': 316,\n","             'now': 522,\n","             'bone': 12,\n","             'my': 4079,\n","             'bones,': 15,\n","             'flesh:': 26,\n","             'she': 875,\n","             'Woman,': 9,\n","             'Man.': 1,\n","             'Therefore': 392,\n","             'leave': 103,\n","             'father': 406,\n","             'mother,': 72,\n","             'cleave': 28,\n","             'wife:': 20,\n","             'flesh.': 38,\n","             'both': 334,\n","             'naked,': 12,\n","             'wife,': 119,\n","             'ashamed.': 17,\n","             'Now': 593,\n","             'serpent': 15,\n","             'more': 518,\n","             'subtil': 3,\n","             'than': 475,\n","             'any': 881,\n","             'Yea,': 96,\n","             'Ye': 274,\n","             'garden?': 1,\n","             'woman': 232,\n","             'serpent,': 10,\n","             'We': 194,\n","             'trees': 95,\n","             'garden:': 3,\n","             'it,': 587,\n","             'neither': 762,\n","             'ye': 3527,\n","             'touch': 43,\n","             'lest': 201,\n","             'die:': 33,\n","             'For': 1654,\n","             'doth': 183,\n","             'know': 664,\n","             'thereof,': 308,\n","             'then': 741,\n","             'your': 1736,\n","             'eyes': 271,\n","             'opened,': 20,\n","             'as': 3182,\n","             'gods,': 84,\n","             'knowing': 36,\n","             'food,': 11,\n","             'eyes,': 122,\n","             'desired': 43,\n","             'wise,': 40,\n","             'did': 838,\n","             'eat,': 61,\n","             'husband': 60,\n","             'with': 5801,\n","             'her;': 37,\n","             'eat.': 44,\n","             'knew': 163,\n","             'naked;': 3,\n","             'sewed': 2,\n","             'fig': 40,\n","             'leaves': 11,\n","             'together,': 146,\n","             'themselves': 261,\n","             'aprons.': 1,\n","             'heard': 559,\n","             'voice': 323,\n","             'walking': 28,\n","             'cool': 2,\n","             'day:': 77,\n","             'wife': 165,\n","             'hid': 105,\n","             'presence': 78,\n","             'amongst': 2,\n","             'garden.': 2,\n","             'him,': 1938,\n","             'Where': 67,\n","             'art': 422,\n","             'thou?': 63,\n","             'thy': 4367,\n","             'afraid,': 38,\n","             'myself.': 14,\n","             'Who': 264,\n","             'told': 257,\n","             'thee': 1510,\n","             'wast': 63,\n","             'naked?': 1,\n","             'Hast': 33,\n","             'eaten': 68,\n","             'whereof': 67,\n","             'shouldest': 66,\n","             'eat?': 8,\n","             'gavest': 33,\n","             'me,': 1174,\n","             'me': 1748,\n","             'What': 312,\n","             'this': 2199,\n","             'hast': 1001,\n","             'done?': 22,\n","             'beguiled': 5,\n","             'Because': 193,\n","             'done': 398,\n","             'this,': 106,\n","             'cursed': 32,\n","             'belly': 26,\n","             'go,': 88,\n","             'days': 536,\n","             'life:': 38,\n","             'enmity': 6,\n","             'between': 226,\n","             'bruise': 6,\n","             'head,': 80,\n","             'heel.': 1,\n","             'Unto': 51,\n","             'greatly': 60,\n","             'sorrow': 39,\n","             'conception;': 1,\n","             'children;': 14,\n","             'desire': 88,\n","             'husband,': 23,\n","             'thee.': 486,\n","             'hearkened': 72,\n","             'thee,': 1170,\n","             'Thou': 563,\n","             'sake;': 7,\n","             'Thorns': 2,\n","             'thistles': 2,\n","             'thee;': 206,\n","             'sweat': 2,\n","             'bread,': 91,\n","             'return': 179,\n","             'ground;': 20,\n","             'taken:': 11,\n","             'art,': 10,\n","             'return.': 11,\n","             \"wife's\": 11,\n","             'Eve;': 1,\n","             'mother': 117,\n","             'living.': 17,\n","             'coats': 8,\n","             'skins,': 14,\n","             'clothed': 69,\n","             'become': 133,\n","             'us,': 297,\n","             'evil:': 27,\n","             'now,': 131,\n","             'hand,': 291,\n","             'take': 683,\n","             'live': 101,\n","             'ever:': 82,\n","             'sent': 598,\n","             'Eden,': 6,\n","             'whence': 57,\n","             'taken.': 15,\n","             'drove': 11,\n","             'man;': 46,\n","             'placed': 11,\n","             'at': 1459,\n","             'Cherubims,': 1,\n","             'flaming': 9,\n","             'sword': 167,\n","             'turned': 248,\n","             'way,': 141,\n","             'way': 379,\n","             'life.': 73,\n","             'Eve': 2,\n","             'wife;': 27,\n","             'conceived,': 22,\n","             'bare': 171,\n","             'Cain,': 9,\n","             'gotten': 21,\n","             'LORD.': 587,\n","             'again': 348,\n","             'brother': 186,\n","             'Abel.': 2,\n","             'Abel': 9,\n","             'keeper': 17,\n","             'sheep,': 67,\n","             'Cain': 11,\n","             'tiller': 1,\n","             'process': 5,\n","             'time': 425,\n","             'came': 1859,\n","             'pass,': 374,\n","             'offering': 361,\n","             'Abel,': 4,\n","             'firstlings': 6,\n","             'flock': 51,\n","             'fat': 100,\n","             'respect': 30,\n","             'offering:': 73,\n","             'respect.': 1,\n","             'wroth,': 17,\n","             'countenance': 28,\n","             'fell.': 2,\n","             'Why': 132,\n","             'wroth?': 1,\n","             'why': 138,\n","             'fallen?': 1,\n","             'If': 474,\n","             'doest': 32,\n","             'well,': 27,\n","             'accepted?': 1,\n","             'if': 1087,\n","             'sin': 274,\n","             'lieth': 56,\n","             'door.': 10,\n","             'desire,': 11,\n","             'talked': 39,\n","             'brother:': 13,\n","             'rose': 124,\n","             'against': 1587,\n","             'brother,': 97,\n","             'slew': 166,\n","             'brother?': 12,\n","             'not:': 127,\n","             'Am': 12,\n","             \"brother's\": 33,\n","             'keeper?': 1,\n","             'blood': 272,\n","             'crieth': 15,\n","             'opened': 97,\n","             'mouth': 219,\n","             'receive': 155,\n","             'hand;': 53,\n","             'When': 348,\n","             'tillest': 1,\n","             'henceforth': 27,\n","             'yield': 28,\n","             'strength;': 5,\n","             'fugitive': 2,\n","             'vagabond': 3,\n","             'LORD,': 1308,\n","             'My': 226,\n","             'punishment': 23,\n","             'can': 211,\n","             'bear.': 2,\n","             'driven': 46,\n","             'earth;': 76,\n","             'hid;': 2,\n","             'come': 1476,\n","             'findeth': 25,\n","             'slay': 103,\n","             'me.': 520,\n","             'whosoever': 129,\n","             'slayeth': 5,\n","             'vengeance': 31,\n","             'sevenfold.': 2,\n","             'mark': 19,\n","             'finding': 10,\n","             'kill': 105,\n","             'dwelt': 207,\n","             'Nod,': 1,\n","             'Eden.': 1,\n","             'Enoch:': 2,\n","             'builded': 33,\n","             'city,': 250,\n","             'son,': 265,\n","             'Enoch.': 1,\n","             'Enoch': 8,\n","             'born': 128,\n","             'Irad:': 1,\n","             'Irad': 1,\n","             'begat': 225,\n","             'Mehujael:': 1,\n","             'Mehujael': 1,\n","             'Methusael:': 1,\n","             'Methusael': 1,\n","             'Lamech.': 2,\n","             'Lamech': 7,\n","             'wives:': 6,\n","             'Adah,': 1,\n","             'other': 415,\n","             'Zillah.': 1,\n","             'Adah': 6,\n","             'Jabal:': 1,\n","             'such': 232,\n","             'dwell': 313,\n","             'tents,': 24,\n","             'cattle.': 12,\n","             'Jubal:': 1,\n","             'handle': 11,\n","             'harp': 8,\n","             'organ.': 2,\n","             'Zillah,': 2,\n","             'Tubalcain,': 1,\n","             'instructer': 1,\n","             'artificer': 1,\n","             'brass': 33,\n","             'iron:': 12,\n","             'sister': 51,\n","             'Tubalcain': 1,\n","             'Naamah.': 1,\n","             'wives,': 46,\n","             'Hear': 81,\n","             'voice;': 22,\n","             'wives': 61,\n","             'Lamech,': 3,\n","             'hearken': 104,\n","             'speech:': 2,\n","             'slain': 126,\n","             'wounding,': 1,\n","             'young': 275,\n","             'hurt.': 13,\n","             'avenged': 16,\n","             'sevenfold,': 2,\n","             'truly': 28,\n","             'seventy': 56,\n","             'again;': 17,\n","             'Seth:': 2,\n","             'God,': 933,\n","             'she,': 8,\n","             'appointed': 101,\n","             'another': 240,\n","             'slew.': 1,\n","             'Seth,': 2,\n","             'son;': 24,\n","             'Enos:': 2,\n","             'began': 138,\n","             'men': 1065,\n","             'book': 111,\n","             'Adam.': 1,\n","             'likeness': 29,\n","             'Male': 1,\n","             'them;': 264,\n","             'created.': 4,\n","             'lived': 49,\n","             'hundred': 501,\n","             'thirty': 151,\n","             'years,': 83,\n","             'son': 1525,\n","             'likeness,': 2,\n","             'image;': 2,\n","             'begotten': 23,\n","             'Seth': 4,\n","             'eight': 61,\n","             'sons': 821,\n","             'daughters:': 21,\n","             'nine': 38,\n","             'died.': 37,\n","             'five': 312,\n","             'Enos': 4,\n","             'seven': 421,\n","             'twelve': 125,\n","             'ninety': 23,\n","             'Cainan:': 1,\n","             'Cainan': 4,\n","             'fifteen': 20,\n","             'years': 301,\n","             'Mahalaleel:': 1,\n","             'Mahalaleel': 4,\n","             'forty': 147,\n","             'ten': 222,\n","             'sixty': 9,\n","             'Jared:': 1,\n","             'Jared': 4,\n","             'Methuselah:': 1,\n","             'walked': 94,\n","             'Methuselah': 4,\n","             'three': 419,\n","             'God:': 201,\n","             'not;': 103,\n","             'eighty': 3,\n","             'son:': 31,\n","             'Noah,': 21,\n","             'comfort': 51,\n","             'concerning': 220,\n","             'toil': 3,\n","             'hands,': 82,\n","             'cursed.': 4,\n","             'Noah': 26,\n","             'old:': 13,\n","             'Shem,': 8,\n","             'Ham,': 10,\n","             'Japheth.': 3,\n","             'daughters': 137,\n","             'That': 330,\n","             'fair;': 4,\n","             'chose.': 1,\n","             'spirit': 221,\n","             'always': 46,\n","             'strive': 15,\n","             'yet': 546,\n","             'twenty': 263,\n","             'years.': 63,\n","             'There': 208,\n","             'giants': 4,\n","             'those': 436,\n","             'days;': 29,\n","             'that,': 110,\n","             'men,': 281,\n","             'children': 1559,\n","             'mighty': 241,\n","             'old,': 65,\n","             'renown.': 1,\n","             'wickedness': 63,\n","             'imagination': 14,\n","             'thoughts': 39,\n","             'heart': 461,\n","             'only': 152,\n","             'evil': 407,\n","             'continually.': 18,\n","             'repented': 29,\n","             'grieved': 23,\n","             'heart.': 88,\n","             'destroy': 230,\n","             'beast,': 50,\n","             'fowls': 51,\n","             'repenteth': 3,\n","             'grace': 114,\n","             'Noah:': 2,\n","             'just': 62,\n","             'perfect': 66,\n","             'generations,': 25,\n","             'God.': 482,\n","             'sons,': 158,\n","             'corrupt': 24,\n","             'filled': 132,\n","             'violence.': 5,\n","             'looked': 96,\n","             'corrupt;': 1,\n","             'corrupted': 13,\n","             'end': 233,\n","             'me;': 174,\n","             'violence': 34,\n","             'through': 419,\n","             'Make': 43,\n","             'ark': 177,\n","             'gopher': 1,\n","             'wood;': 7,\n","             'rooms': 4,\n","             'ark,': 33,\n","             'pitch': 14,\n","             'within': 164,\n","             'pitch.': 3,\n","             'fashion': 11,\n","             'of:': 5,\n","             'length': 65,\n","             'cubits,': 48,\n","             'breadth': 74,\n","             'fifty': 112,\n","             'height': 49,\n","             'cubits.': 27,\n","             'A': 233,\n","             'window': 7,\n","             'cubit': 32,\n","             'finish': 9,\n","             'above;': 5,\n","             'door': 126,\n","             'side': 263,\n","             'lower,': 2,\n","             'second,': 18,\n","             'stories': 2,\n","             'And,': 105,\n","             'I,': 97,\n","             'even': 1200,\n","             'do': 1118,\n","             'flood': 23,\n","             'flesh,': 103,\n","             'heaven;': 38,\n","             'establish': 43,\n","             'covenant;': 7,\n","             'thou,': 183,\n","             \"sons'\": 21,\n","             'sort': 9,\n","             'alive': 30,\n","             'female.': 4,\n","             'alive.': 17,\n","             'food': 31,\n","             'eaten,': 15,\n","             'gather': 142,\n","             'Noah;': 1,\n","             'according': 725,\n","             'so': 917,\n","             'he.': 22,\n","             'Come': 78,\n","             'house': 1221,\n","             'ark;': 6,\n","             'seen': 226,\n","             'righteous': 155,\n","             'generation.': 22,\n","             'clean': 57,\n","             'by': 2435,\n","             'sevens,': 2,\n","             'female:': 2,\n","             'beasts': 101,\n","             'two,': 13,\n","             'air': 15,\n","             'female;': 1,\n","             'cause': 277,\n","             'nights;': 3,\n","             'substance': 31,\n","             'off': 358,\n","             'six': 175,\n","             'old': 248,\n","             'in,': 92,\n","             'flood.': 4,\n","             'beasts,': 41,\n","             'clean,': 18,\n","             'fowls,': 2,\n","             'female,': 6,\n","             'Noah.': 1,\n","             'pass': 382,\n","             'hundredth': 3,\n","             'year': 212,\n","             \"Noah's\": 2,\n","             'month,': 116,\n","             'seventeenth': 5,\n","             'fountains': 14,\n","             'broken': 131,\n","             'up,': 268,\n","             'windows': 18,\n","             'opened.': 7,\n","             'nights.': 4,\n","             'selfsame': 15,\n","             'entered': 99,\n","             'Japheth,': 2,\n","             'They,': 2,\n","             'bird': 16,\n","             'sort.': 2,\n","             'him:': 309,\n","             'shut': 86,\n","             'in.': 29,\n","             'increased,': 6,\n","             'lift': 83,\n","             'prevailed,': 3,\n","             'increased': 33,\n","             'prevailed': 24,\n","             'exceedingly': 18,\n","             'high': 292,\n","             'hills,': 23,\n","             'heaven,': 202,\n","             'covered.': 8,\n","             'Fifteen': 1,\n","             'cubits': 99,\n","             'upward': 15,\n","             'prevail;': 3,\n","             'mountains': 84,\n","             'died': 86,\n","             'fowl,': 9,\n","             'man:': 48,\n","             'All': 191,\n","             'land,': 293,\n","             'destroyed': 106,\n","             'things,': 180,\n","             'remained': 43,\n","             'alive,': 20,\n","             'ark.': 10,\n","             'days.': 86,\n","             'remembered': 47,\n","             'ark:': 4,\n","             'wind': 70,\n","             'asswaged;': 1,\n","             'stopped,': 5,\n","             'restrained;': 1,\n","             'returned': 135,\n","             'continually:': 4,\n","             'abated.': 2,\n","             'Ararat.': 1,\n","             'decreased': 1,\n","             'continually': 38,\n","             'until': 340,\n","             'tenth': 75,\n","             'month:': 10,\n","             'tops': 10,\n","             'seen.': 11,\n","             'made:': 7,\n","             'raven,': 1,\n","             'fro,': 4,\n","             'dried': 32,\n","             'Also': 95,\n","             'dove': 10,\n","             'abated': 4,\n","             'no': 1297,\n","             'rest': 175,\n","             'sole': 11,\n","             'foot,': 19,\n","             'her,': 258,\n","             'pulled': 7,\n","             'stayed': 20,\n","             'evening;': 2,\n","             'lo,': 108,\n","             'olive': 35,\n","             'leaf': 10,\n","             'pluckt': 1,\n","             'off:': 20,\n","             'dove;': 1,\n","             'more.': 57,\n","             'year,': 69,\n","             'removed': 65,\n","             'covering': 36,\n","             'looked,': 40,\n","             'dry.': 4,\n","             'twentieth': 32,\n","             'dried.': 2,\n","             'spake': 508,\n","             'Go': 167,\n","             'Bring': 43,\n","             'breed': 2,\n","             'forth,': 90,\n","             'Every': 58,\n","             'kinds,': 3,\n","             'altar': 188,\n","             'LORD;': 259,\n","             'offered': 124,\n","             'burnt': 341,\n","             'offerings': 127,\n","             'altar.': 45,\n","             'smelled': 2,\n","             'sweet': 97,\n","             'savour;': 2,\n","             'heart,': 197,\n","             'curse': 65,\n","             \"man's\": 115,\n","             'youth;': 8,\n","             'smite': 104,\n","             'living,': 9,\n","             'done.': 40,\n","             'While': 28,\n","             'remaineth,': 4,\n","             'seedtime': 1,\n","             'harvest,': 15,\n","             'cold': 12,\n","             'heat,': 9,\n","             'summer': 22,\n","             'winter,': 2,\n","             'night': 121,\n","             'cease.': 11,\n","             'fear': 265,\n","             'dread': 7,\n","             'fishes': 10,\n","             'sea;': 26,\n","             'hand': 828,\n","             'delivered.': 9,\n","             'liveth': 19,\n","             'meat': 213,\n","             'you;': 86,\n","             'things.': 76,\n","             'lives': 17,\n","             'require;': 1,\n","             'require': 26,\n","             'Whoso': 30,\n","             'sheddeth': 2,\n","             'blood,': 95,\n","             'shed:': 1,\n","             'you,': 827,\n","             ...})"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"e8usMg7Z4w5F","executionInfo":{"status":"ok","timestamp":1636390160208,"user_tz":-60,"elapsed":809,"user":{"displayName":"Christian Kauth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3-aO9Ea873CYWBJu-flxslweQj0T7KM2WKzfx=s64","userId":"03504929373507274108"}}},"source":["sc.stop()"],"execution_count":27,"outputs":[]}]}